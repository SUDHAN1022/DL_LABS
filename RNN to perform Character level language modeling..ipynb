{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UPGXgFSll6Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def initialize_parameters(vocab_size, hidden_layer_size):\n",
        "\n",
        "    parameters = {}\n",
        "    parameters[\"Whh\"] = np.random.randn(\n",
        "        hidden_layer_size, hidden_layer_size) * 0.01\n",
        "    parameters[\"Wxh\"] = np.random.randn(hidden_layer_size, vocab_size) * 0.01\n",
        "    parameters[\"b\"] = np.zeros((hidden_layer_size, 1))\n",
        "    parameters[\"Why\"] = np.random.randn(vocab_size, hidden_layer_size) * 0.01\n",
        "    parameters[\"c\"] = np.zeros((vocab_size, 1))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def initialize_adam(parameters):\n",
        "\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "    v = {}\n",
        "    s = {}\n",
        "\n",
        "    for param_name in parameters_names:\n",
        "        v[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
        "        s[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
        "\n",
        "    return v, s\n",
        "\n",
        "\n",
        "def initialize_rmsprop(parameters):\n",
        "\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "    s = {}\n",
        "\n",
        "    for param_name in parameters_names:\n",
        "        s[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def softmax(z):\n",
        "\n",
        "    e_z = np.exp(z)\n",
        "    probs = e_z / np.sum(e_z)\n",
        "\n",
        "    return probs\n",
        "\n",
        "\n",
        "def rnn_forward(x, y, h_prev, parameters):\n",
        "\n",
        "    # Retrieve parameters\n",
        "    Wxh, Whh, b = parameters[\"Wxh\"], parameters[\"Whh\"], parameters[\"b\"]\n",
        "    Why, c = parameters[\"Why\"], parameters[\"c\"]\n",
        "\n",
        "    # Initialize inputs, hidden state, output, and probabilities dictionaries\n",
        "    xs, hs, os, probs = {}, {}, {}, {}\n",
        "\n",
        "    # Initialize x0 to zero vector\n",
        "    xs[0] = np.zeros((vocab_size, 1))\n",
        "\n",
        "    # Initialize loss and assigns h_prev to last hidden state in hs\n",
        "    loss = 0\n",
        "    hs[-1] = np.copy(h_prev)\n",
        "\n",
        "    # Forward pass: loop over all characters of the name\n",
        "    for t in range(len(x)):\n",
        "        # Convert to one-hot vector\n",
        "        if t > 0:\n",
        "            xs[t] = np.zeros((vocab_size, 1))\n",
        "            xs[t][x[t]] = 1\n",
        "        # Hidden state\n",
        "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + b)\n",
        "        # Logits\n",
        "        os[t] = np.dot(Why, hs[t]) + c\n",
        "        # Probs\n",
        "        probs[t] = softmax(os[t])\n",
        "        # Loss\n",
        "        loss -= np.log(probs[t][y[t], 0])\n",
        "\n",
        "    cache = (xs, hs, probs)\n",
        "\n",
        "    return loss, cache\n",
        "\n",
        "\n",
        "def smooth_loss(loss, current_loss):\n",
        "\n",
        "    return 0.999 * loss + 0.001 * current_loss\n",
        "\n",
        "\n",
        "def clip_gradients(gradients, max_value):\n",
        "\n",
        "    for grad in gradients.keys():\n",
        "        np.clip(gradients[grad], -max_value, max_value, out=gradients[grad])\n",
        "\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def rnn_backward(y, parameters, cache):\n",
        "\n",
        "    # Retrieve xs, hs, and probs\n",
        "    xs, hs, probs = cache\n",
        "\n",
        "    # Initialize all gradients to zero\n",
        "    dh_next = np.zeros_like(hs[0])\n",
        "\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "    grads = {}\n",
        "    for param_name in parameters_names:\n",
        "        grads[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
        "\n",
        "    # Iterate over all time steps in reverse order starting from Tx\n",
        "    for t in reversed(range(len(xs))):\n",
        "        dy = np.copy(probs[t])\n",
        "        dy[y[t]] -= 1\n",
        "        grads[\"dWhy\"] += np.dot(dy, hs[t].T)\n",
        "        grads[\"dc\"] += dy\n",
        "        dh = np.dot(parameters[\"Why\"].T, dy) + dh_next\n",
        "        dhraw = (1 - hs[t] ** 2) * dh\n",
        "        grads[\"dWhh\"] += np.dot(dhraw, hs[t - 1].T)\n",
        "        grads[\"dWxh\"] += np.dot(dhraw, xs[t].T)\n",
        "        grads[\"db\"] += dhraw\n",
        "        dh_next = np.dot(parameters[\"Whh\"].T, dhraw)\n",
        "        # Clip the gradients using [-5, 5] as the interval\n",
        "        grads = clip_gradients(grads, 5)\n",
        "    # Get the last hidden state\n",
        "    h_prev = hs[len(xs) - 1]\n",
        "\n",
        "    return grads, h_prev\n",
        "\n",
        "\n",
        "def update_parameters_with_adam(\n",
        "        parameters, grads, v, s, t, learning_rate, beta1=0.9, beta2=0.999,\n",
        "        epsilon=1e-8):\n",
        "\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "    v_corrected = {}\n",
        "    s_corrected = {}\n",
        "\n",
        "    for param_name in parameters_names:\n",
        "        # Update the moving average of first gradient and squared gradient\n",
        "        v[\"d\" + param_name] = beta1 * v[\"d\" + param_name] +\\\n",
        "            (1 - beta1) * grads[\"d\" + param_name]\n",
        "        s[\"d\" + param_name] = beta2 * s[\"d\" + param_name] +\\\n",
        "            (1 - beta2) * np.square(grads[\"d\" + param_name])\n",
        "\n",
        "        # Compute the corrected-bias estimate of the moving averages\n",
        "        v_corrected[\"d\" + param_name] = v[\"d\" + param_name] / (1 - beta1**t)\n",
        "        s_corrected[\"d\" + param_name] = s[\"d\" + param_name] / (1 - beta2**t)\n",
        "\n",
        "        # update parameters\n",
        "        parameters[param_name] -= (learning_rate *\n",
        "                                   v_corrected[\"d\" + param_name])\\\n",
        "            / (np.sqrt(s_corrected[\"d\" + param_name] + epsilon))\n",
        "\n",
        "    return parameters, v, s\n",
        "\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    for param in parameters.keys():\n",
        "        parameters[param] -= learning_rate * grads[\"d\" + param]\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def update_parameters_with_rmsprop(\n",
        "        parameters, grads, s, beta=0.9, learning_rate=0.001, epsilon=1e-8):\n",
        "\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "\n",
        "    for param_name in parameters_names:\n",
        "        # Update exponential weighted average of squared gradients\n",
        "        s[\"d\" + param_name] = beta * s[\"d\" + param_name] +\\\n",
        "            (1 - beta) * np.square(grads[\"d\" + param_name])\n",
        "\n",
        "        # Update parameters\n",
        "        parameters[param_name] -= (learning_rate * grads[\"d\" + param_name])\\\n",
        "            / (np.sqrt(s[\"d\" + param_name] + epsilon))\n",
        "\n",
        "    return parameters, s\n",
        "\n",
        "\n",
        "def sample(parameters, idx_to_chars, chars_to_idx, n):\n",
        "\n",
        "    # Retrienve parameters, shapes, and vocab size\n",
        "    Whh, Wxh, b = parameters[\"Whh\"], parameters[\"Wxh\"], parameters[\"b\"]\n",
        "    Why, c = parameters[\"Why\"], parameters[\"c\"]\n",
        "    n_h, n_x = Wxh.shape\n",
        "    vocab_size = c.shape[0]\n",
        "\n",
        "    # Initialize a0 and x1 to zero vectors\n",
        "    h_prev = np.zeros((n_h, 1))\n",
        "    x = np.zeros((n_x, 1))\n",
        "\n",
        "    # Initialize empty sequence\n",
        "    indices = []\n",
        "    idx = -1\n",
        "    counter = 0\n",
        "    while (counter <= n and idx != chars_to_idx[\"\\n\"]):\n",
        "        # Fwd propagation\n",
        "        h = np.tanh(np.dot(Whh, h_prev) + np.dot(Wxh, x) + b)\n",
        "        o = np.dot(Why, h) + c\n",
        "        probs = softmax(o)\n",
        "\n",
        "        # Sample the index of the character using generated probs distribution\n",
        "        idx = np.random.choice(vocab_size, p=probs.ravel())\n",
        "\n",
        "        # Get the character of the sampled index\n",
        "        char = idx_to_chars[idx]\n",
        "\n",
        "        # Add the char to the sequence\n",
        "        indices.append(idx)\n",
        "\n",
        "        # Update a_prev and x\n",
        "        h_prev = np.copy(h)\n",
        "        x = np.zeros((n_x, 1))\n",
        "        x[idx] = 1\n",
        "\n",
        "        counter += 1\n",
        "    sequence = \"\".join([idx_to_chars[idx] for idx in indices if idx != 0])\n",
        "\n",
        "    return sequence\n",
        "\n",
        "\n",
        "def model(\n",
        "        file_path, chars_to_idx, idx_to_chars, hidden_layer_size, vocab_size,\n",
        "        num_epochs=10, learning_rate=0.01):\n",
        "\n",
        "    # Get the data\n",
        "    with open(file_path) as f:\n",
        "        data = f.readlines()\n",
        "    examples = [x.lower().strip() for x in data]\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(vocab_size, hidden_layer_size)\n",
        "\n",
        "    # Initialize Adam parameters\n",
        "    s = initialize_rmsprop(parameters)\n",
        "\n",
        "    # Initialize loss\n",
        "    smoothed_loss = -np.log(1 / vocab_size) * 7\n",
        "\n",
        "    # Initialize hidden state h0 and overall loss\n",
        "    h_prev = np.zeros((hidden_layer_size, 1))\n",
        "    overall_loss = []\n",
        "\n",
        "    # Iterate over number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\033[1m\\033[94mEpoch {epoch}\")\n",
        "        print(f\"\\033[1m\\033[92m=======\")\n",
        "\n",
        "        # Sample one name\n",
        "        print(f\"\"\"Sampled name: {sample(parameters, idx_to_chars, chars_to_idx,\n",
        "            10).capitalize()}\"\"\")\n",
        "        print(f\"Smoothed loss: {smoothed_loss:.4f}\\n\")\n",
        "\n",
        "        # Shuffle examples\n",
        "        np.random.shuffle(examples)\n",
        "\n",
        "        # Iterate over all examples (SGD)\n",
        "        for example in examples:\n",
        "            x = [None] + [chars_to_idx[char] for char in example]\n",
        "            y = x[1:] + [chars_to_idx[\"\\n\"]]\n",
        "            # Fwd pass\n",
        "            loss, cache = rnn_forward(x, y, h_prev, parameters)\n",
        "            # Compute smooth loss\n",
        "            smoothed_loss = smooth_loss(smoothed_loss, loss)\n",
        "            # Bwd passA\n",
        "            grads, h_prev = rnn_backward(y, parameters, cache)\n",
        "            # Update parameters\n",
        "            parameters, s = update_parameters_with_rmsprop(\n",
        "                parameters, grads, s)\n",
        "\n",
        "        overall_loss.append(smoothed_loss)\n",
        "\n",
        "    return parameters, overall_loss"
      ],
      "metadata": {
        "id": "5KYHbQlv4Vd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLKRfg23NtBh"
      },
      "outputs": [],
      "source": [
        "def rnn_forward(x, y, h_prev, parameters):\n",
        "    Wxh, Whh, b = parameters[\"Wxh\"], parameters[\"Whh\"], parameters[\"b\"]\n",
        "    Why, c = parameters[\"Why\"], parameters[\"c\"]\n",
        "    xs, hs, os, probs = {}, {}, {}, {}\n",
        "    xs[0] = np.zeros((vocab_size, 1))\n",
        "    loss = 0\n",
        "    hs[-1] = np.copy(h_prev)\n",
        "    for t in range(len(x)):\n",
        "        if t > 0:\n",
        "            xs[t] = np.zeros((vocab_size, 1))\n",
        "            xs[t][x[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + b)\n",
        "        os[t] = np.dot(Why, hs[t]) + c\n",
        "        probs[t] = softmax(os[t])\n",
        "        loss -= np.log(probs[t][y[t], 0])\n",
        "    cache = (xs, hs, probs)\n",
        "    return loss, cache"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradients(gradients, max_value):\n",
        "    for grad in gradients.keys():\n",
        "        np.clip(gradients[grad], -max_value, max_value, out=gradients[grad])\n",
        "    return gradients\n",
        "\n",
        "def rnn_backward(y, parameters, cache):\n",
        "    # Retrieve xs, hs, and probs\n",
        "    xs, hs, probs = cache\n",
        "    \n",
        "    # Initialize all gradients to zero\n",
        "    dh_next = np.zeros_like(hs[0])\n",
        "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
        "    grads = {}\n",
        "    for param_name in parameters_names:\n",
        "        grads[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
        "    \n",
        "    # Iterate over all time steps in reverse order starting from Tx\n",
        "    for t in reversed(range(len(xs))):\n",
        "        dy = np.copy(probs[t])\n",
        "        dy[y[t]] -= 1\n",
        "        grads[\"dWhy\"] += np.dot(dy, hs[t].T)\n",
        "        grads[\"dc\"] += dy\n",
        "        dh = np.dot(parameters[\"Why\"].T, dy) + dh_next\n",
        "        dhraw = (1 - hs[t] ** 2) * dh\n",
        "        grads[\"dWhh\"] += np.dot(dhraw, hs[t - 1].T)\n",
        "        grads[\"dWxh\"] += np.dot(dhraw, xs[t].T)\n",
        "        grads[\"db\"] += dhraw\n",
        "        dh_next = np.dot(parameters[\"Whh\"].T, dhraw)\n",
        "        # Clip the gradients using [-5, 5] as the interval\n",
        "        grads = clip_gradients(grads, 5)\n",
        "    \n",
        "    # Get the last hidden state\n",
        "    h_prev = hs[len(xs) - 1]\n",
        "    return grads, h_prev"
      ],
      "metadata": {
        "id": "fkotAXNTUbtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(parameters, idx_to_chars, chars_to_idx, n):\n",
        "    # Retrienve parameters, shapes, and vocab size\n",
        "    Whh, Wxh, b = parameters[\"Whh\"], parameters[\"Wxh\"], parameters[\"b\"]\n",
        "    Why, c = parameters[\"Why\"], parameters[\"c\"]\n",
        "    n_h, n_x = Wxh.shape\n",
        "    vocab_size = c.shape[0]\n",
        "    \n",
        "    # Initialize a0 and x1 to zero vectors\n",
        "    h_prev = np.zeros((n_h, 1))\n",
        "    x = np.zeros((n_x, 1))\n",
        "    # Initialize empty sequence\n",
        "    indices = []\n",
        "    idx = -1\n",
        "    counter = 0\n",
        "    while (counter <= n and idx != chars_to_idx[\"\\n\"]):\n",
        "        # Fwd propagation\n",
        "        h = np.tanh(np.dot(Whh, h_prev) + np.dot(Wxh, x) + b)\n",
        "        o = np.dot(Why, h) + c\n",
        "        probs = softmax(o)\n",
        "        \n",
        "        # Sample the index of the character using generated probs distribution\n",
        "        idx = np.random.choice(vocab_size, p=probs.ravel())\n",
        "        \n",
        "        # Get the character of the sampled index\n",
        "        char = idx_to_chars[idx]\n",
        "    \n",
        "        # Add the char to the sequence\n",
        "        indices.append(idx)\n",
        "        \n",
        "        # Update a_prev and x\n",
        "        h_prev = np.copy(h)\n",
        "        x = np.zeros((n_x, 1))\n",
        "        x[idx] = 1\n",
        "        \n",
        "        counter += 1\n",
        "    sequence = \"\".join([idx_to_chars[idx] for idx in indices if idx != 0])\n",
        "    return sequence"
      ],
      "metadata": {
        "id": "pBTlkkRDUKzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(\n",
        "        file_path, chars_to_idx, idx_to_chars, hidden_layer_size, vocab_size,\n",
        "        num_epochs=10, learning_rate=0.01):\n",
        "    # Get the data\n",
        "    with open(file_path) as f:\n",
        "        data = f.readlines()\n",
        "    examples = [x.lower().strip() for x in data]\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(vocab_size, hidden_layer_size)\n",
        "\n",
        "    # Initialize Adam parameters\n",
        "    s = initialize_rmsprop(parameters)\n",
        "\n",
        "    # Initialize loss\n",
        "    smoothed_loss = -np.log(1 / vocab_size) * 7\n",
        "\n",
        "    # Initialize hidden state h0 and overall loss\n",
        "    h_prev = np.zeros((hidden_layer_size, 1))\n",
        "    overall_loss = []\n",
        "\n",
        "    # Iterate over number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\033[1m\\033[94mEpoch {epoch}\")\n",
        "        print(f\"\\033[1m\\033[92m=======\")\n",
        "\n",
        "        # Sample one name\n",
        "        print(f\"\"\"Sampled name: {sample(parameters, idx_to_chars, chars_to_idx,\n",
        "            10).capitalize()}\"\"\")\n",
        "        print(f\"Smoothed loss: {smoothed_loss:.4f}\\n\")\n",
        "\n",
        "        # Shuffle examples\n",
        "        np.random.shuffle(examples)\n",
        "\n",
        "        # Iterate over all examples (SGD)\n",
        "        for example in examples:\n",
        "            x = [None] + [chars_to_idx[char] for char in example]\n",
        "            y = x[1:] + [chars_to_idx[\"\\n\"]]\n",
        "            # Fwd pass\n",
        "            loss, cache = rnn_forward(x, y, h_prev, parameters)\n",
        "            # Compute smooth loss\n",
        "            smoothed_loss = smooth_loss(smoothed_loss, loss)\n",
        "            # Bwd pass\n",
        "            grads, h_prev = rnn_backward(y, parameters, cache)\n",
        "            # Update parameters\n",
        "            parameters, s = update_parameters_with_rmsprop(\n",
        "                parameters, grads, s)\n",
        "\n",
        "        overall_loss.append(smoothed_loss)\n",
        "    return parameters, overall_loss"
      ],
      "metadata": {
        "id": "3x02sBVSUlc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load names\n",
        "data = open(\"rnn.txt\", \"r\").read()\n",
        "\n",
        "# Convert characters to lower case\n",
        "data = data.lower()\n",
        "\n",
        "# Construct vocabulary using unique characters, sort it in ascending order,\n",
        "# then construct two dictionaries that maps character to index and index to\n",
        "# characters.\n",
        "chars = list(sorted(set(data)))\n",
        "chars_to_idx = {ch:i for i, ch in enumerate(chars)}\n",
        "idx_to_chars = {i:ch for ch, i in chars_to_idx.items()}\n",
        "\n",
        "# Get the size of the data and vocab size\n",
        "data_size = len(data)\n",
        "vocab_size = len(chars_to_idx)\n",
        "print(f\"There are {data_size} characters and {vocab_size} unique characters.\")\n",
        "\n",
        "# Fitting the model\n",
        "parameters, loss = model(\"/content/rnn.txt\", chars_to_idx, idx_to_chars, 100, vocab_size, 10, 0.01)\n",
        "\n",
        "# Plotting the loss\n",
        "plt.plot(range(len(loss)), loss)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Smoothed loss\");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-M6gQEBmUqoa",
        "outputId": "ab22d7d5-eb76-40b6-8440-42dad11fb9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 412 characters and 29 unique characters.\n",
            "\u001b[1m\u001b[94mEpoch 0\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Sanw k;ri;w\n",
            "Smoothed loss: 23.5711\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 1\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Wc\n",
            "Smoothed loss: 24.4266\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 2\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Yrkdlom\n",
            "Smoothed loss: 25.2074\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 3\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Mk bipel l'\n",
            "Smoothed loss: 25.9690\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 4\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Ey;htet  \n",
            "Smoothed loss: 26.7150\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 5\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Wp'fuoa ac.\n",
            "Smoothed loss: 27.4421\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 6\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Rykaot\n",
            "Smoothed loss: 28.1502\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 7\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: Lpbyu.ubx\n",
            "Smoothed loss: 28.8441\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 8\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: \n",
            "Smoothed loss: 29.5180\n",
            "\n",
            "\u001b[1m\u001b[94mEpoch 9\n",
            "\u001b[1m\u001b[92m=======\n",
            "Sampled name: 'bakuxea ,s\n",
            "Smoothed loss: 30.1683\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KUy1yQCAUumF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}